name: Database Backup

on:
  schedule:
    - cron: '0 2 * * *'
  workflow_dispatch:

jobs:
  backup-production:
    runs-on: ubuntu-latest
    environment: production
    steps:
      - uses: actions/checkout@v4
      
      - name: Install PostgreSQL client
        run: |
          sudo apt-get update
          sudo apt-get install -y postgresql-client
      
      - name: Create backup
        env:
          SUPABASE_PRODUCTION_DB_URL: ${{ secrets.SUPABASE_PRODUCTION_DB_URL }}
        run: |
          TIMESTAMP=$(date +%Y%m%d_%H%M%S)
          BACKUP_FILE="backup_${TIMESTAMP}.sql"
          
          pg_dump "$SUPABASE_PRODUCTION_DB_URL" \
            --format=plain \
            --no-owner \
            --no-acl \
            --exclude-schema=extensions \
            --exclude-schema=graphql \
            --exclude-schema=graphql_public \
            > "$BACKUP_FILE"
          
          gzip "$BACKUP_FILE"
          echo "BACKUP_FILE=${BACKUP_FILE}.gz" >> $GITHUB_ENV
      
      - name: Upload to S3
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_REGION: ${{ secrets.AWS_REGION }}
          S3_BUCKET: ${{ secrets.BACKUP_S3_BUCKET }}
        run: |
          aws s3 cp "$BACKUP_FILE" "s3://${S3_BUCKET}/production/${BACKUP_FILE}" --storage-class GLACIER_IR
      
      - name: Cleanup old backups
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_REGION: ${{ secrets.AWS_REGION }}
          S3_BUCKET: ${{ secrets.BACKUP_S3_BUCKET }}
        run: |
          CUTOFF_DATE=$(date -d '30 days ago' +%Y%m%d)
          aws s3 ls "s3://${S3_BUCKET}/production/" | while read -r line; do
            FILE=$(echo $line | awk '{print $4}')
            FILE_DATE=$(echo $FILE | grep -oP '\d{8}' | head -1)
            if [ ! -z "$FILE_DATE" ] && [ "$FILE_DATE" -lt "$CUTOFF_DATE" ]; then
              echo "Deleting old backup: $FILE"
              aws s3 rm "s3://${S3_BUCKET}/production/${FILE}"
            fi
          done

  backup-staging:
    runs-on: ubuntu-latest
    environment: staging
    steps:
      - uses: actions/checkout@v4
      
      - name: Install PostgreSQL client
        run: |
          sudo apt-get update
          sudo apt-get install -y postgresql-client
      
      - name: Create backup
        env:
          SUPABASE_STAGING_DB_URL: ${{ secrets.SUPABASE_STAGING_DB_URL }}
        run: |
          TIMESTAMP=$(date +%Y%m%d_%H%M%S)
          BACKUP_FILE="backup_${TIMESTAMP}.sql"
          
          pg_dump "$SUPABASE_STAGING_DB_URL" \
            --format=plain \
            --no-owner \
            --no-acl \
            --exclude-schema=extensions \
            --exclude-schema=graphql \
            --exclude-schema=graphql_public \
            > "$BACKUP_FILE"
          
          gzip "$BACKUP_FILE"
          echo "BACKUP_FILE=${BACKUP_FILE}.gz" >> $GITHUB_ENV
      
      - name: Upload to S3
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_REGION: ${{ secrets.AWS_REGION }}
          S3_BUCKET: ${{ secrets.BACKUP_S3_BUCKET }}
        run: |
          aws s3 cp "$BACKUP_FILE" "s3://${S3_BUCKET}/staging/${BACKUP_FILE}" --storage-class STANDARD_IA
